# -*- coding: utf-8 -*-
"""Automatic_mask_generator_SAM_Vessels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MQOIOPfMvO6hUY2BBcoU40kxfSbgKTDa
"""

# Copyright (c) Meta Platforms, Inc. and affiliates.

"""# Automatically generating object masks with SAM

Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.

The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes.
"""

from IPython.display import display, HTML
display(HTML(
"""
<a target="_blank" href="https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
"""
))

"""## Environment Set-up

If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'.
"""

using_colab = True

if using_colab:
    import torch
    import torchvision
    print("PyTorch version:", torch.__version__)
    print("Torchvision version:", torchvision.__version__)
    print("CUDA is available:", torch.cuda.is_available())
    import sys
    !{sys.executable} -m pip install opencv-python matplotlib
    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'

    !mkdir images
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg

    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

"""## Set-up"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import cv2

def show_anns(anns):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)

    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        img[m] = color_mask
    ax.imshow(img)

"""## Import Vessel Images"""

from google.colab import drive
drive.mount('/content/drive')

import glob, os, gzip
import nibabel as nib, numpy as np
!pip install SimpleITK
import SimpleITK as sitk

STORAGE_PATH = os.path.join("drive", "MyDrive", "BrainVeins_Main" , "BrainVeins", "datafolder")
DATA_PATH = os.path.join(STORAGE_PATH, "data")
scandirs = sorted(glob.glob(os.path.join(DATA_PATH, "*/*/step03_SWI/TE1/*")))

print(scandirs)

import nibabel as nib
import os

# List of .nii.gz files
niifiles = []

for fileimg in scandirs:
  if not fileimg.endswith('leftbox.nii.gz') and not fileimg.endswith('rightbox.nii.gz'):
    niifiles.append(fileimg)

# List to hold the new .nii files
new_niifiles = []

# Function to convert .nii.gz files to .nii files
def convert_gz_to_nii(gz_file):
    try:
        # Load the .nii.gz file
        img = nib.load(gz_file)

        # Create a new file path by replacing .nii.gz with .nii
        new_file_path = gz_file.replace('.nii.gz', '.nii')

        # Create a Nifti1Image with the data and affine from the loaded image
        nifti_img = nib.Nifti1Image(img.get_fdata(), img.affine)

        # Save the Nifti1Image to the new file path
        nib.save(nifti_img, new_file_path)

        # Return the new file path
        return new_file_path

    except RuntimeError:
        print(f"Could not convert file {gz_file} - not recognized as a NIFTI file")
        return None

# Convert each .nii.gz file in the list and store new paths
for nii_gz_file in niifiles:
    new_file_path = convert_gz_to_nii(nii_gz_file)
    if new_file_path is not None:
        new_niifiles.append(new_file_path)

print(new_niifiles)

"""##Use MIPS to Visualize Vessel Structures"""

import SimpleITK as sitk
import nibabel as nib
import matplotlib.pyplot as plt

# Function to perform MIP and display the image
def mip_and_display(nii_file):
    try:
        # Check if the file can be loaded with nibabel
        nib.load(nii_file)

        # Load the image with SimpleITK
        img = sitk.ReadImage(nii_file)

        # Perform MIP
        mip = sitk.MaximumProjection(img, projectionDimension=0)

        # Convert to numpy array for visualization
        mip_arr = sitk.GetArrayFromImage(mip)

        # Display the image
        plt.imshow(mip_arr, cmap="gray")
        plt.show()

    except Exception as e:
        print(f"Could not perform MIP on {nii_file} due to error: {e}")

# Perform MIP and display for each .nii file in the list
for nii_file in new_niifiles:
    mip_and_display(nii_file)

import SimpleITK as sitk
import nibabel as nib
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# Function to perform MIP and return the image array
def mip_and_return(nii_file):
    try:
        # Check if the file can be loaded with nibabel
        nib.load(nii_file)

        # Load the image with SimpleITK
        img = sitk.ReadImage(nii_file)

        # Perform MIP
        mip = sitk.MaximumProjection(img, projectionDimension=0)

        # Convert to numpy array for visualization
        mip_arr = sitk.GetArrayFromImage(mip)

        return mip_arr

    except Exception as e:
        print(f"Could not perform MIP on {nii_file} due to error: {e}")
        return None




# Set up the 3D plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Perform MIP and display for each .nii file in the list
for i, nii_file in enumerate(new_niifiles):
    mip_arr = mip_and_return(nii_file)

    if mip_arr is not None:
        x, y = np.meshgrid(np.arange(mip_arr.shape[1]), np.arange(mip_arr.shape[0]))
        z = np.ones_like(x) * i  # adjust this to set the z coordinates properly

        # Flatten the arrays
        x = x.flatten()
        y = y.flatten()
        z = z.flatten()
        mip_arr = mip_arr.flatten()

        # Normalize the data for visualization purpose
        mip_arr = (mip_arr - np.min(mip_arr)) / (np.max(mip_arr) - np.min(mip_arr))

        # Only plot points above a certain intensity threshold
        mask = mip_arr > 0.5

        ax.scatter(x[mask], y[mask], z[mask], c=mip_arr[mask], cmap='jet')

plt.show()

import numpy as np
import matplotlib.pyplot as plt
import cv2

# Iterate over each MIP image and display it
for index, mip_image in enumerate(min_images):
    # Create a new figure with a larger size
    plt.figure(figsize=(8, 8))

    # Convert the image to 8-bit unsigned integer
    mip_image = (mip_image * 255).astype(np.uint8)

    # Apply adaptive thresholding
    block_size = 7  # Set the block size for adaptive thresholding (must be odd and greater than 1)
    constant = 5  # Set the constant for adaptive thresholding
    binary_image = cv2.adaptiveThreshold(mip_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, constant)

    if(index == 13):
      # Display the binary image with adjusted contrast
      plt.imshow(binary_image, cmap='gray', vmin=np.min(min_images), vmax=np.max(min_images))
      plt.title(f'Adaptive Thesholding {index}')
      plt.axis('off')
      plt.show()



import numpy as np
import matplotlib.pyplot as plt

def mip(img):
    return np.max(img, axis=2)

# Assuming `img` is your 3D image
mip_img = mip(img)

# Display the result
plt.imshow(mip_img, cmap='gray')
plt.show()

realimg = []
synimg = []
synthetic = scandirs
for img in synthetic:
  if img.endswith('SWI_TE1.nii.gz'):
    realimg.append(img)



# Right and left box images

for image in synthetic:
  if not image.endswith('leftbox.nii.gz') and not image.endswith('rightbox.nii.gz'):
    synimg.append(image)


print(synimg)

print(len(synimg))

import SimpleITK as sitk
from PIL import Image
import os
import numpy as np

def convert_nii_to_jpg(nii_filepath, output_dir):
    # Load NIfTI image using SimpleITK
    nii_image = sitk.ReadImage(nii_filepath)

    # Get the image data as a numpy array
    image_data = sitk.GetArrayViewFromImage(nii_image)

    # Normalize the image data to 0-255 range
    normalized_data = ((image_data - np.min(image_data)) / (np.max(image_data) - np.min(image_data))) * 255

    # Convert the data type to uint8
    normalized_data = normalized_data.astype('uint8')

    # Convert the numpy array to PIL Image
    image = Image.fromarray(normalized_data[0])

    # Save the image as JPEG in the output directory
    filename = os.path.splitext(os.path.basename(nii_filepath))[0] + '.jpg'
    jpg_filepath = os.path.join(output_dir, filename)
    image.save(jpg_filepath)

# Example usage
nii_filepaths = synimg
output_dir = 'drive/MyDrive/BrainVeins_Main/BrainVeins/datafolder/output'

for nii_filepath in nii_filepaths:
    convert_nii_to_jpg(nii_filepath, output_dir)

# Read the first image from output_dir using cv2.imread()
image_filename = os.listdir(output_dir)[13]
image_path = os.path.join(output_dir, image_filename)
image = cv2.imread(image_path)

#image = cv2.imread('images/dog.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)



plt.figure(figsize=(8,8))
plt.imshow(image)
plt.axis('off')
plt.show()

"""## Automatic mask generation

To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended.
"""

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

sam_checkpoint = "sam_vit_h_4b8939.pth"
model_type = "vit_h"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

mask_generator = SamAutomaticMaskGenerator(sam)

"""To generate masks, just run `generate` on an image."""

masks = mask_generator.generate(image)

"""Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:
* `segmentation` : the mask
* `area` : the area of the mask in pixels
* `bbox` : the boundary box of the mask in XYWH format
* `predicted_iou` : the model's own prediction for the quality of the mask
* `point_coords` : the sampled input point that generated this mask
* `stability_score` : an additional measure of mask quality
* `crop_box` : the crop of the image used to generate this mask in XYWH format
"""

print(len(masks))
print(masks[0].keys())

"""Show all the masks overlayed on the image."""

plt.figure(figsize=(8,8))
plt.imshow(image)
show_anns(masks)
plt.axis('off')
plt.show()

"""## Automatic mask generation options

There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:
"""

mask_generator_2 = SamAutomaticMaskGenerator(
    model=sam,
    points_per_side=32,
    pred_iou_thresh=0.86,
    stability_score_thresh=0.92,
    crop_n_layers=1,
    crop_n_points_downscale_factor=2,
    min_mask_region_area=100,  # Requires open-cv to run post-processing
)

mask_generator_3 = SamAutomaticMaskGenerator(
    model=sam,
    points_per_side=32,
    pred_iou_thresh=0.10,
    stability_score_thresh=0.92,
    crop_n_layers=1,
    crop_n_points_downscale_factor=2,
    min_mask_region_area=100,  # Requires open-cv to run post-processing
)

masks2 = mask_generator_2.generate(image)

masks3 = mask_generator_3.generate(image)

len(masks2)

len(masks3)

plt.figure(figsize=(8,8))
plt.imshow(image)
show_anns(masks3)
plt.axis('off')
plt.show()

plt.figure(figsize=(8,8))
plt.imshow(image)
show_anns(masks2)
plt.axis('off')
plt.show()

